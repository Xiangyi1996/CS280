{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "Visdom python client failed to establish socket to get messages from the server. This feature is optional and can be disabled by initializing Visdom with `use_incoming_socket=False`, which will prevent waiting for this request to timeout.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from AllCNN import *\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from utils import Visualizer\n",
    "from load_data import *\n",
    "vis = Visualizer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cuda = True\n",
    "train_batch_size = 32\n",
    "test_batch_size = 124\n",
    "best_loss = float(\"inf\")\n",
    "best_epoch = -1\n",
    "best_acc = 0\n",
    "dataset_path = './cifar-100/class1'\n",
    "#gsync_save = True\n",
    "\n",
    "\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)\n",
    "\n",
    "\n",
    "\n",
    "cuda = cuda and torch.cuda.is_available()\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = CustomData(root=dataset_path, train=True,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size,\n",
    "                                          shuffle=True, **kwargs)\n",
    "\n",
    "testset = CustomData(root=dataset_path, train=False,transform=transform)\n",
    "                                       \n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size,\n",
    "                                         shuffle=False, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AllConvNet(\n",
       "  (conv1): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv4): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv7): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
       "  (conv9): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-521ce8729340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#model = model.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcheckpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoints/AllCNN_.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workstation1/Transfer L/AllCNN.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n",
      "'>' not supported between instances of 'float' and 'NoneType'\n"
     ]
    }
   ],
   "source": [
    "model = AllConvNet(3)\n",
    "model.init_weights()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "#num_ftrs = model.conv9.in_channels\n",
    "#model.conv9 = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "#model = model.to(device)\n",
    "checkpath = 'checkpoints/AllCNN_.pth'\n",
    "weight = model.load(checkpath)['state_dict']\n",
    "\n",
    "weight.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200, 250, 300], gamma=0.1)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "        #print(output.shape, target.shape)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = 100. * correct /len(train_loader.dataset)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def test(epoch, best_loss,best_acc):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct /len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset), test_acc))\n",
    "\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(model, \"best.pt\")\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    return test_loss, test_acc, best_loss, best_acc\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "    scheduler.step()\n",
    "    train_loss, train_acc = train(epoch)\n",
    "    vis.plot('train_loss',train_loss)\n",
    "    vis.plot('train_acc',train_acc)\n",
    "    test_loss, test_acc, best_loss, best_acc = test(epoch, best_loss, best_acc)\n",
    "    vis.plot('test_loss',test_loss)\n",
    "    vis.plot('test_acc',test_acc)\n",
    "    vis.plot('best_loss',best_loss)\n",
    "    vis.plot('best_acc',best_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_gpu]",
   "language": "python",
   "name": "conda-env-pytorch_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
